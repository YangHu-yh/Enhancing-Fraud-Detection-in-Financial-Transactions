{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9013505,"sourceType":"datasetVersion","datasetId":5430856},{"sourceId":9995327,"sourceType":"datasetVersion","datasetId":6151932},{"sourceId":9995434,"sourceType":"datasetVersion","datasetId":6151992}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# **Reading the file**","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/processed-dataset/test_balanced_dataset_with_updated_columns.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_original = pd.read_csv('/kaggle/input/credit-card-transactions-dataset/credit_card_transactions.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Data Exploration**","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> **The following code first explores the dataset using the get_df_info function, which provides an overview of the dataset's shape, columns, data types, unique values, null values, duplicate rows, and descriptive statistics.**","metadata":{}},{"cell_type":"code","source":"def get_df_info(df):\n    print(\"\\n\\033[1mShape of DataFrame:\\033[0m \", df.shape)\n    print(\"\\n\\033[1mColumns in DataFrame:\\033[0m \", df.columns.to_list())\n    print(\"\\n\\033[1mData types of columns:\\033[0m\\n\", df.dtypes)\n    \n    print(\"\\n\\033[1mInformation about DataFrame:\\033[0m\")\n    df.info()\n    \n    print(\"\\n\\033[1mNumber of unique values in each column:\\033[0m\")\n    for col in df.columns:\n        print(f\"\\033[1m{col}\\033[0m: {df[col].nunique()}\")\n        \n    print(\"\\n\\033[1mNull values in columns:\\033[0m\")\n    null_counts = df.isnull().sum()\n    null_columns = null_counts[null_counts > 0]\n    if len(null_columns) > 0:\n        for col, count in null_columns.items():\n            print(f\"\\033[1m{col}\\033[0m: {count}\")\n    else:\n        print(\"There are no null values in the DataFrame.\")\n    \n    print(\"\\n\\033[1mNumber of duplicate rows:\\033[0m \", df.duplicated().sum())\n    \n    print(\"\\n\\033[1mDescriptive statistics of DataFrame:\\033[0m\\n\",)\n    return df.describe().transpose()\n\n# Call the function\nget_df_info(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here's a brief explanation of the output:\n\n**Data Overview**\nThe dataset contains 1,296,675 rows and 24 columns. The columns include transaction details such as date, time, credit card number, merchant information, transaction amount, and location data.\n\n**Data Types**\nThe data types of the columns vary, with 6 float64, 6 int64, and 12 object (string) columns.\n\n**Unique Values**\nThe number of unique values in each column ranges from 2 (gender, is_fraud) to 1,296,675 (trans_num, unix_time). This suggests that some columns have a high cardinality, while others have a low cardinality.\n\n**Null Values**\nThere are null values in the merch_zipcode column, with 195,973 missing values.\n\n**Duplicate Rows**\nThere are no duplicate rows in the dataset.\n\n**Descriptive Statistics**\nThe descriptive statistics provide an overview of the central tendency and variability of the numerical columns. For example, the mean transaction amount is around 70,  with a standard deviation of around 160. The mean latitude and longitude values suggest that the transactions are concentrated in a specific region.\n\nOverall, this output provides a comprehensive overview of the dataset's structure, content, and distribution. It highlights the presence of null values, unique values, and duplicate rows, which can inform data preprocessing and feature engineering decisions.\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Machine Learning using EvalML**","metadata":{}},{"cell_type":"markdown","source":"\nThe command pip install evalml is used to install the EvalML library, which is an automated machine learning (AutoML) library for Python. EvalML provides a simple and efficient way to perform automated machine learning tasks, including data preprocessing, model selection, hyperparameter tuning, and model evaluation.\n\nHere's a brief explanation of the EvalML library:\n\n**Key Features:**\n\n* **Automated Machine Learning:** EvalML automates the machine learning process, from data preprocessing to model evaluation.\n\n* **Data Preprocessing:** EvalML provides automatic data preprocessing, including handling missing values, encoding categorical variables, and feature scaling.\n\n* **Model Selection:** EvalML supports a wide range of machine learning algorithms and automatically selects the best model for the problem.\n\n* **Hyperparameter Tuning:** EvalML performs hyperparameter tuning to optimize model performance.\n\n* **Model Evaluation:** EvalML provides comprehensive model evaluation metrics and visualizations.\n\n\n**Benefits:**\n\n* **Saves Time:** EvalML automates the machine learning process, saving time and effort.\n\n* **Improves Performance:** EvalML's automated hyperparameter tuning and model selection improve model performance.\n\n* **Easy to Use:** EvalML has a simple and intuitive API, making it easy to use for both beginners and experienced machine learning practitioners.\n\nBy installing EvalML using pip install evalml, you can leverage these features and benefits to streamline your machine learning workflow and improve your model's performance.\n\n","metadata":{}},{"cell_type":"code","source":"pip install evalml","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install evalml packaging --upgrade","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evalml\nfrom evalml import AutoMLSearch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting Features and target\nX = df.drop(['is_fraud'], axis=1)\ny = df['is_fraud']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use df_original\n# Splitting Features and target\nX_original = df_original.drop(['is_fraud'], axis=1)\ny_original = df_original['is_fraud']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_original, X_test_original, y_train_original, y_test_original = evalml.preprocessing.split_data(X_original, y_original, problem_type='binary')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = evalml.preprocessing.split_data(X, y, problem_type='binary')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThis line of code splits the dataset into training and testing sets using EvalML's split_data function. Here's a breakdown of the parameters:\n* X: The feature data (independent variables)\n* y: The target data (dependent variable)\n* problem_type='binary': Specifies that this is a binary classification problem (i.e., the target variable has two classes)\n\nThe function returns four arrays:\n\n* X_train: The training feature data.\n* X_test: The testing feature data.\n* y_train: The training target data.\n* y_test: The testing target data.","metadata":{}},{"cell_type":"code","source":"automl = AutoMLSearch(X_train=X_train, y_train=y_train, problem_type='binary', max_iterations=50)\nautoml.search()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The output shows the results of the AutoML search process. Here's a breakdown of the output:\n* The search process evaluated multiple models, and the results are stored in a dictionary with batch numbers as keys (1, 2, ...).\n* Each batch contains a dictionary with model names as keys and their corresponding cross-validation scores as values.\n* The scores are likely accuracy or F1 scores, given the problem type is binary classification.\n* The models are a combination of different algorithms (e.g., Random Forest, LightGBM, Extra Trees, Elastic Net, XGBoost, Logistic Regression) with various preprocessors and transformers (e.g., Label Encoder, Select Columns By Type Transformer, DateTime Featurizer, Imputer, One Hot Encoder, Undersampler).\n* The \"Total time of batch\" key shows the total time taken for each batch.\n\nFrom the output, we can see that:\n* Batch 1 evaluated a single model, Random Forest Classifier, with a score of 48.26.\n* Batch 2 evaluated five different models, with scores ranging from 23.45 (Extra Trees Classifier) to 27.82 (XGBoost Classifier).\n\nThe AutoML search process aims to find the best-performing model for the given problem. You can access the best-performing model using automl.best_pipeline, which will return the model with the highest score.\n\n","metadata":{}},{"cell_type":"code","source":"# Get the rankings with details\nrankings = automl.rankings\nprint(rankings)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here's a breakdown of the output:\nThe rankings attribute returns a pandas DataFrame containing the results of the AutoML search process.\n\nThe columns are:\n* id: A unique identifier for each pipeline.\n* pipeline_name: The name of the pipeline, including the algorithm and preprocessors/transformers used.\n* search_order: The order in which the pipeline was evaluated during the search process.\n* ranking_score: A score used to rank the pipelines, with lower values indicating better performance.\n* mean_cv_score: The mean cross-validation score for each pipeline.\n* standard_deviation_cv_score: The standard deviation of the cross-validation scores for each pipeline.\n* percent_better_than_baseline: The percentage improvement over the baseline model.\n* high_variance_cv: A boolean indicating whether the cross-validation scores have high variance.\n* parameters: The hyperparameters used for each pipeline.\n\nThe rows are sorted by the ranking_score, with the best-performing pipeline at the top.\n\nFrom the output, we can see that:\n\n* The top-performing pipeline is the LightGBM Classifier with a ranking score of 0.036276 and an 82.58% improvement over the baseline.\n* The worst-performing pipeline is the Mode Baseline Binary Classification Pipeline with a ranking score of 0.208199 and no improvement over the baseline.\n\n\nYou can access the best-performing pipeline using automl.best_pipeline, which will return the pipeline with the lowest ranking score.\n\n","metadata":{}},{"cell_type":"code","source":"automl.best_pipeline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe best_pipeline attribute returns the best-performing pipeline from the AutoML search process. \n\n**Here's a breakdown of the output:**\n\nThe pipeline is a complex graph of components, including:\n\n* Preprocessors: Label Encoder, Select Columns By Type Transformer, Drop Columns Transformer, DateTime Featurizer, Imputer, One Hot Encoder\n* Balancer: Undersampler\n* Classifier: LightGBM Classifier\n\nThe pipeline has a large number of hyperparameters, which are tuned to optimize performance.\n\nThe hyperparameters are stored in the parameters dictionary, which contains settings for each component.\n\nThe random_seed parameter is set to 0, which ensures reproducibility of the results.","metadata":{}},{"cell_type":"code","source":"best_pipeline=automl.best_pipeline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"automl.describe_pipeline(automl.rankings.iloc[0][\"id\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The describe_pipeline method provides a detailed description of the pipeline, including:\n\n**Pipeline Name:** LightGBM Classifier with various preprocessors and transformers.\n\n**Problem Type:** Binary classification.\n\n**Model Family:** LightGBM.\n\n**Pipeline Steps:** A list of 13 components, including:\n\n* Label Encoder (x3)\n* Select Columns By Type Transformer\n* Drop Columns Transformer\n* DateTime Featurizer\n* Imputer (x2)\n* Select Columns Transformer (x2)\n* One Hot Encoder\n* Undersampler\n* LightGBM Classifier\n\n**Hyperparameters:** Detailed settings for each component, including:\n* Column types and exclusions\n* Imputation strategies\n* Encoding schemes\n* Sampling ratios\n* LightGBM hyperparameters (e.g., learning rate, n_estimators, max_depth)\n\n**Training:**\nTotal training time (including CV): 27.0 seconds\n\n**Cross Validation:**\n* Metrics: Log Loss, Binary MCC, Gini, AUC, Precision, F1, Balanced Accuracy, Binary Accuracy\n* Mean and standard deviation of each metric across folds\n* Coefficient of variation for each metric\n\nThis detailed description provides insight into the pipeline's architecture, hyperparameters, and performance.\n\n","metadata":{}},{"cell_type":"code","source":"# Evaluate on hold out data\nbest_pipeline.score(X_test, y_test, objectives=[\"auc\",\"f1\",\"Precision\",\"Recall\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe score method evaluates the performance of the best pipeline on the holdout data (X_test, y_test) using the specified objectives:\n\n* AUC (Area Under the Receiver Operating Characteristic Curve)\n* F1 (F1 score, the harmonic mean of precision and recall)\n* Precision (the ratio of true positives to true positives plus false positives)\n* Recall (the ratio of true positives to true positives plus false negatives)\n\nThe output is an OrderedDict with the objective names as keys and the corresponding scores as values.\n\nHere's a brief interpretation of the scores:\n\n* AUC: 0.997, indicating excellent performance, with a high degree of separation between positive and negative classes.\n* F1: 0.819, indicating good balance between precision and recall.\n* Precision: 0.840, indicating a high ratio of true positives to true positives plus false positives.\n* Recall: 0.798, indicating a good ratio of true positives to true positives plus false negatives.\n\nThese scores suggest that the best pipeline is performing well on the holdout data, with excellent AUC and good balance between precision and recall.\n\n","metadata":{}},{"cell_type":"code","source":"# Evaluate on hold out data\ny_pred = best_pipeline.predict(X_test)\ny_pred_proba = best_pipeline.predict_proba(X_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Classification Report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Mechanics of classification_report:\nclassification_report from scikit-learn generates a report on the classification performance. \n\nHere's a breakdown of the output:\n* Precision: The ratio of true positives (TP) to the sum of true positives and false positives (FP) for each class.\n* Recall: The ratio of true positives (TP) to the sum of true positives and false negatives (FN) for each class.\n* F1-score: The harmonic mean of precision and recall for each class.\n* Support: The number of instances in each class.\n\nFor the given output:\n\n**Class 0 (likely the negative class):**\n* Precision: 1.00 (perfect precision)\n* Recall: 1.00 (perfect recall)\n* F1-score: 1.00 (perfect F1-score)\n* Support: 257834 instances\n\n\n**Class 1 (likely the positive class):**\n* Precision: 0.84\n* Recall: 0.80\n* F1-score: 0.82\n* Support: 1501 instances\n\n**The report also provides averages:**\n* Accuracy: The overall accuracy of the classifier.\n* Macro avg: The average of the precision, recall, and F1-score for all classes, weighted equally.\n* Weighted avg: The average of the precision, recall, and F1-score for all classes, weighted by the support (number of instances) of each class.\n\n\nIn this case, the classifier performs perfectly on the negative class (Class 0) but has some errors on the positive class (Class 1). The macro average and weighted average provide a summary of the performance across both classes.\n\n","metadata":{}},{"cell_type":"code","source":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate F1 score\nfrom sklearn.metrics import f1_score\nf1 = f1_score(y_test, y_pred, average='weighted')  # or 'macro' / 'micro' depending on your use case\nprint(\"F1 Score:\", f1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nThe confusion matrix provides a summary of the classifier's predictions against the actual true labels. Here's a breakdown of the output:\n\n**Interpretation:**\n* True Negatives (TN): 257606 instances correctly predicted as Class 0 (negative class)\n* False Positives (FP): 303 instances incorrectly predicted as Class 1 (positive class) when they were actually Class 0\n* False Negatives (FN): 228 instances incorrectly predicted as Class 0 when they were actually Class 1\n* True Positives (TP): 1198 instances correctly predicted as Class 1 (positive class)\n\n**The confusion matrix helps identify:**\n* Errors in prediction (FP, FN)\n* Accuracy of the classifier (TN, TP)\n* Class imbalance (difference in support between classes)\n\nIn this case, the classifier performs well on the negative class (Class 0) but has some errors on the positive class (Class 1), with a relatively low number of true positives compared to false negatives.\n\n","metadata":{}},{"cell_type":"markdown","source":"# **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"The AutoML process successfully identified a high-performing pipeline for the binary classification problem, utilizing a LightGBM classifier with various preprocessors and transformers. \n\nThe best pipeline achieved excellent performance on the holdout data, with:\n\n* AUC: 0.997\n* F1-score: 0.819\n* Precision: 0.840\n* Recall: 0.798\n\nThe classification report and confusion matrix revealed:\n\n* High accuracy on the negative class (Class 0)\n* Good performance on the positive class (Class 1), with some room for improvement\n\nThe AutoML process demonstrated its effectiveness in:\n\n* Automating the machine learning workflow\n* Identifying a high-performing pipeline\n* Providing insights into the classification performance\n\nHowever, there is still room for improvement, particularly in:\n* Addressing class imbalance\n* Further optimizing hyperparameters\n* Exploring additional algorithms and techniques\n\nOverall, the AutoML process provided valuable insights and a solid foundation for further development and improvement.","metadata":{}},{"cell_type":"markdown","source":"# **Kindly UPVOTE**","metadata":{}},{"cell_type":"markdown","source":"Hope you find this notebook useful. Please Upvote","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}